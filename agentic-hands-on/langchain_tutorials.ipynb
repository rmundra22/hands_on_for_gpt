{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3436d41-410f-495e-b9b7-f352e0d7b657",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /opt/conda/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from groq) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/jovyan/.local/lib/python3.11/site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/jovyan/.local/lib/python3.11/site-packages (from groq) (2.12.5)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /home/jovyan/.local/lib/python3.11/site-packages (from groq) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->groq) (3.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jovyan/.local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/jovyan/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/jovyan/.local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/jovyan/.local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/jovyan/.local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install groq\n",
    "!pip install python-dotenv\n",
    "!pip install feedparser textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9806301-d07e-4ac0-a4b5-8a2f710dc2a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# cwd = os.getcwd()\n",
    "# print(cwd)\n",
    "\n",
    "# os.chdir(\"/home/jovyan/fm-assist-volume/practice_gpt/hands_on_for_gpt/agentic-hands-on\") # Changes the current working directory\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b61981-6235-49c1-969e-b4218c83921f",
   "metadata": {},
   "source": [
    "# 1. Minimal Agent\n",
    "\n",
    "This is our simplest â€œagentâ€ using groq-api:\n",
    "\n",
    "* Has a role\n",
    "* Takes input\n",
    "* Produces reasoning-based output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d8c418a-7a6e-438a-a519-9231ca113ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To explain Kafka vs API-based communication, let's break down each concept and then compare them.\n",
      "\n",
      "**API-based Communication:**\n",
      "\n",
      "API (Application Programming Interface) is a set of defined rules that enable different applications, systems, or services to communicate with each other. It's a request-response model where a client (e.g., a web application) sends a request to a server, and the server responds with the requested data.\n",
      "\n",
      "Here's a step-by-step overview of API-based communication:\n",
      "\n",
      "1. **Request**: The client sends an HTTP request (e.g., GET, POST, PUT, DELETE) to the server with the required data.\n",
      "2. **Server Processing**: The server processes the request, performs the necessary actions, and retrieves the required data.\n",
      "3. **Response**: The server sends an HTTP response back to the client with the requested data.\n",
      "\n",
      "API-based communication is widely used for real-time communication between applications, such as:\n",
      "\n",
      "* Web applications communicating with backend servers\n",
      "* Mobile apps interacting with servers\n",
      "* Microservices communicating with each other\n",
      "\n",
      "**Kafka-based Communication:**\n",
      "\n",
      "Apache Kafka is a distributed streaming platform that enables high-throughput, fault-tolerant, and scalable data processing. It's designed for handling large amounts of data and provides a messaging system for communication between applications.\n",
      "\n",
      "Here's a step-by-step overview of Kafka-based communication:\n",
      "\n",
      "1. **Producer**: An application (producer) sends data to a Kafka topic, which is a stream of related data.\n",
      "2. **Kafka Broker**: The data is stored in a Kafka broker, which is a server that manages the data and ensures its availability.\n",
      "3. **Consumer**: Another application (consumer) subscribes to the Kafka topic and reads the data from the broker.\n",
      "4. **Processing**: The consumer processes the data and performs the necessary actions.\n",
      "\n",
      "Kafka-based communication is commonly used for:\n",
      "\n",
      "* Real-time data processing and analytics\n",
      "* Event-driven architectures\n",
      "* Microservices communication\n",
      "* Log aggregation and processing\n",
      "\n",
      "**Comparison:**\n",
      "\n",
      "Now, let's compare Kafka and API-based communication:\n",
      "\n",
      "* **Request-Response Model**: API-based communication follows a request-response model, whereas Kafka-based communication is based on a publish-subscribe model.\n",
      "* **Data Processing**: API-based communication is designed for request-response interactions, whereas Kafka-based communication is optimized for high-throughput, real-time data processing.\n",
      "* **Scalability**: Kafka is designed for horizontal scaling and can handle large amounts of data, whereas API-based communication can become bottlenecked as the number of requests increases.\n",
      "* **Fault Tolerance**: Kafka provides built-in fault tolerance and redundancy, whereas API-based communication relies on the underlying infrastructure for fault tolerance.\n",
      "* **Coupling**: API-based communication tightly couples the client and server, whereas Kafka-based communication decouples the producer and consumer, allowing for greater flexibility and scalability.\n",
      "\n",
      "**When to use each:**\n",
      "\n",
      "* Use API-based communication when:\n",
      "\t+ You need a request-response model for real-time interactions.\n",
      "\t+ You're building a web application or mobile app that requires direct communication with a server.\n",
      "* Use Kafka-based communication when:\n",
      "\t+ You need to handle large amounts of data and require high-throughput processing.\n",
      "\t+ You're building an event-driven architecture or a microservices-based system.\n",
      "\t+ You require a scalable, fault-tolerant, and decoupled communication system.\n",
      "\n",
      "In summary, API-based communication is suitable for request-response interactions, while Kafka-based communication is optimized for high-throughput, real-time data processing and event-driven architectures. The choice between the two ultimately depends on the specific requirements of your application or system.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "class SimpleAgent:\n",
    "    def __init__(self, system_prompt):\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def run(self, user_input):\n",
    "        # use llm-client's chat completion service and models to generate a response\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "agent = SimpleAgent(\n",
    "    system_prompt=\"You are a helpful AI agent that reasons step by step.\"\n",
    ")\n",
    "\n",
    "print(agent.run(\"Explain Kafka vs API based communication\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6c528-86ca-44a7-9e39-94e15f945980",
   "metadata": {},
   "source": [
    "# 2. Stateful Agent (Memory + Iterative Reasoning)\n",
    "\n",
    "Now this agent:\n",
    "\n",
    "* Remembers context\n",
    "* Builds conversation state\n",
    "* Can ask clarifying questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a275c0a9-4a67-4c7c-819d-eb08dfc8d38c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Response from the llm:\n",
    "\n",
    "```\n",
    "ChatCompletion(\n",
    "    id='chatcmpl-c4554bf1-edd2-4c87-98c1-e4595aa5d831', \n",
    "    choices=[\n",
    "        Choice(\n",
    "            finish_reason='stop', \n",
    "            index=0, \n",
    "            logprobs=None, \n",
    "            message=ChatCompletionMessage(\n",
    "                content=\"To help you decide between Kafka and API, ... and I'll help you make an informed decision!\", \n",
    "                role='assistant', \n",
    "                annotations=None, \n",
    "                executed_tools=None, \n",
    "                function_call=None, \n",
    "                reasoning=None, \n",
    "                tool_calls=None\n",
    "            )\n",
    "        )\n",
    "    ], \n",
    "    created=1769932176, \n",
    "    model='llama-3.3-70b-versatile', \n",
    "    object='chat.completion', \n",
    "    mcp_list_tools=None, \n",
    "    service_tier='on_demand', \n",
    "    system_fingerprint='fp_43d97c5965', \n",
    "    usage=CompletionUsage(\n",
    "        completion_tokens=340, \n",
    "        prompt_tokens=56, \n",
    "        total_tokens=396, \n",
    "        completion_time=1.149168506, \n",
    "        completion_tokens_details=None, \n",
    "        prompt_time=0.002955258, \n",
    "        prompt_tokens_details=None, \n",
    "        queue_time=0.008930827, \n",
    "        total_time=1.152123764\n",
    "    ), \n",
    "    usage_breakdown=None, \n",
    "    x_groq=XGroq(\n",
    "        id='req_01kgc2ve1segyrev1pmj1b891k', \n",
    "        debug=None, \n",
    "        seed=831061670, \n",
    "        usage=None\n",
    "    )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3420c08-453e-4a8c-a1ab-c2e06b5ea60b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To help you choose between Kafka and API, I need to understand your use case a bit better. Can you please provide some context about your project? Here are a few clarifying questions to get started:\n",
      "\n",
      "1. What type of data are you trying to handle (e.g., real-time events, batch processing, request-response)?\n",
      "2. What is the volume and velocity of the data (e.g., how much data, how often)?\n",
      "3. Are you building a new system or integrating with existing systems?\n",
      "4. What are your primary concerns (e.g., scalability, reliability, latency, security)?\n",
      "5. Are there any specific technologies or frameworks you're already using or planning to use?\n",
      "\n",
      "Kafka and API are designed for different purposes:\n",
      "\n",
      "* Kafka is a distributed streaming platform designed for high-throughput, fault-tolerant, and scalable data processing. It's often used for event-driven architectures, real-time data processing, and microservices communication.\n",
      "* API (Application Programming Interface) is a set of defined rules that enable different applications to communicate with each other. APIs are typically used for request-response interactions, where a client sends a request to a server, and the server responds with data.\n",
      "\n",
      "By understanding your specific requirements, I can help you determine whether Kafka, API, or a combination of both is the best fit for your project.\n",
      "With a downstream SLA (Service Level Agreement) of 4 hours, it means that your system has a relatively relaxed timeline to process and deliver data to the downstream consumer.\n",
      "\n",
      "Given this context, here are a few follow-up questions to help me better understand your requirements:\n",
      "\n",
      "1. What is the nature of the data being processed (e.g., logs, transactions, sensor readings, messages)?\n",
      "2. How much data are we talking about (e.g., number of records, size of payload)?\n",
      "3. Are there any specific processing requirements, such as data transformation, aggregation, or filtering?\n",
      "4. Are there any upstream systems or producers that will be sending data to your system, and if so, what is their expected volume and velocity?\n",
      "5. Are there any specific reliability or durability requirements for the data (e.g., must it be persisted, can it be lost in case of failure)?\n",
      "\n",
      "Considering the 4-hour SLA, Kafka might still be a good fit if:\n",
      "\n",
      "* You need to handle high-volume data streams\n",
      "* You require a fault-tolerant and scalable architecture\n",
      "* You want to decouple producers and consumers, allowing for greater flexibility and scalability\n",
      "\n",
      "On the other hand, an API-based approach might be more suitable if:\n",
      "\n",
      "* You have a relatively simple, request-response based workflow\n",
      "* You don't expect extremely high volumes of data\n",
      "* You need to provide a more traditional, synchronous interface for data exchange\n",
      "\n",
      "However, if you need to handle large volumes of data or require a more scalable and fault-tolerant architecture, Kafka might be a better choice, even with a 4-hour SLA. Kafka can provide a buffer for incoming data, allowing your system to process it at a more leisurely pace, while still meeting the downstream SLA.\n",
      "\n",
      "Can you provide more details about your specific use case and requirements?\n",
      "agent workflow as chat: \n",
      "\n",
      "[{'content': 'You are a senior system design agent. Ask clarifying questions '\n",
      "             'when needed.',\n",
      "  'role': 'system'},\n",
      " {'content': 'Help me choose Kafka vs API', 'role': 'user'},\n",
      " {'content': 'To help you choose between Kafka and API, I need to understand '\n",
      "             'your use case a bit better. Can you please provide some context '\n",
      "             'about your project? Here are a few clarifying questions to get '\n",
      "             'started:\\n'\n",
      "             '\\n'\n",
      "             '1. What type of data are you trying to handle (e.g., real-time '\n",
      "             'events, batch processing, request-response)?\\n'\n",
      "             '2. What is the volume and velocity of the data (e.g., how much '\n",
      "             'data, how often)?\\n'\n",
      "             '3. Are you building a new system or integrating with existing '\n",
      "             'systems?\\n'\n",
      "             '4. What are your primary concerns (e.g., scalability, '\n",
      "             'reliability, latency, security)?\\n'\n",
      "             \"5. Are there any specific technologies or frameworks you're \"\n",
      "             'already using or planning to use?\\n'\n",
      "             '\\n'\n",
      "             'Kafka and API are designed for different purposes:\\n'\n",
      "             '\\n'\n",
      "             '* Kafka is a distributed streaming platform designed for '\n",
      "             'high-throughput, fault-tolerant, and scalable data processing. '\n",
      "             \"It's often used for event-driven architectures, real-time data \"\n",
      "             'processing, and microservices communication.\\n'\n",
      "             '* API (Application Programming Interface) is a set of defined '\n",
      "             'rules that enable different applications to communicate with '\n",
      "             'each other. APIs are typically used for request-response '\n",
      "             'interactions, where a client sends a request to a server, and '\n",
      "             'the server responds with data.\\n'\n",
      "             '\\n'\n",
      "             'By understanding your specific requirements, I can help you '\n",
      "             'determine whether Kafka, API, or a combination of both is the '\n",
      "             'best fit for your project.',\n",
      "  'role': 'assistant'},\n",
      " {'content': 'Downstream SLA is 4 hours\\n', 'role': 'user'},\n",
      " {'content': 'With a downstream SLA (Service Level Agreement) of 4 hours, it '\n",
      "             'means that your system has a relatively relaxed timeline to '\n",
      "             'process and deliver data to the downstream consumer.\\n'\n",
      "             '\\n'\n",
      "             'Given this context, here are a few follow-up questions to help '\n",
      "             'me better understand your requirements:\\n'\n",
      "             '\\n'\n",
      "             '1. What is the nature of the data being processed (e.g., logs, '\n",
      "             'transactions, sensor readings, messages)?\\n'\n",
      "             '2. How much data are we talking about (e.g., number of records, '\n",
      "             'size of payload)?\\n'\n",
      "             '3. Are there any specific processing requirements, such as data '\n",
      "             'transformation, aggregation, or filtering?\\n'\n",
      "             '4. Are there any upstream systems or producers that will be '\n",
      "             'sending data to your system, and if so, what is their expected '\n",
      "             'volume and velocity?\\n'\n",
      "             '5. Are there any specific reliability or durability requirements '\n",
      "             'for the data (e.g., must it be persisted, can it be lost in case '\n",
      "             'of failure)?\\n'\n",
      "             '\\n'\n",
      "             'Considering the 4-hour SLA, Kafka might still be a good fit if:\\n'\n",
      "             '\\n'\n",
      "             '* You need to handle high-volume data streams\\n'\n",
      "             '* You require a fault-tolerant and scalable architecture\\n'\n",
      "             '* You want to decouple producers and consumers, allowing for '\n",
      "             'greater flexibility and scalability\\n'\n",
      "             '\\n'\n",
      "             'On the other hand, an API-based approach might be more suitable '\n",
      "             'if:\\n'\n",
      "             '\\n'\n",
      "             '* You have a relatively simple, request-response based workflow\\n'\n",
      "             \"* You don't expect extremely high volumes of data\\n\"\n",
      "             '* You need to provide a more traditional, synchronous interface '\n",
      "             'for data exchange\\n'\n",
      "             '\\n'\n",
      "             'However, if you need to handle large volumes of data or require '\n",
      "             'a more scalable and fault-tolerant architecture, Kafka might be '\n",
      "             'a better choice, even with a 4-hour SLA. Kafka can provide a '\n",
      "             'buffer for incoming data, allowing your system to process it at '\n",
      "             'a more leisurely pace, while still meeting the downstream SLA.\\n'\n",
      "             '\\n'\n",
      "             'Can you provide more details about your specific use case and '\n",
      "             'requirements?',\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "from pprint import pprint\n",
    "\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "class MemoryAgent:\n",
    "    def __init__(self, system_prompt):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    def run(self, user_input):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=self.messages,\n",
    "            temperature=0.2\n",
    "        )\n",
    "\n",
    "        assistant_msg = response.choices[0].message.content\n",
    "        \n",
    "        # save the response into a memory building a new context\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        return assistant_msg\n",
    "\n",
    "\n",
    "agent = MemoryAgent(\n",
    "    system_prompt=\"You are a senior system design agent. Ask clarifying questions when needed.\"\n",
    ")\n",
    "\n",
    "print(agent.run(\"Help me choose Kafka vs API\"))\n",
    "print(agent.run(\"Downstream SLA is 4 hours\\n\"))\n",
    "print(f\"agent workflow as chat: \\n\")\n",
    "pprint(agent.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a22765-8835-4c84-b2e8-ccf334edcbe6",
   "metadata": {},
   "source": [
    "# 3. Tool-Calling Agent (True â€œAgenticâ€ Behavior)\n",
    "\n",
    "Agent decides whether to call a tool or think. We can then:\n",
    "\n",
    "* Parse structured JSON\n",
    "* Holds memory (messages)\n",
    "* Registers tools\n",
    "* Runs agent loop (Loop until task completes)\n",
    "* Executes tools\n",
    "* Returns final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87128d86-59e6-40a8-b632-ddc490e8456f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "class ToolAgent:\n",
    "    def __init__(self, model=\"llama-3.3-70b-versatile\"):\n",
    "        self.client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "        self.model = model\n",
    "\n",
    "        self.messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a senior system design agent.\\n\"\n",
    "                    \"- If information is missing, ask clarifying questions.\\n\"\n",
    "                    \"- If enough information is available, call the tool ONCE.\\n\"\n",
    "                    \"- After receiving tool output, provide a FINAL answer.\\n\"\n",
    "                    \"- Do NOT call tools again after a tool result.\"\n",
    "                ) # system_prompt given at the start\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # defining tool-schema clearly\n",
    "        self.tools = [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"choose_transport\",\n",
    "                    \"description\": \"Decide Kafka vs API based on system constraints\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"sla_hours\": {\"type\": \"integer\"},\n",
    "                            \"volume\": {\"type\": \"integer\"},\n",
    "                            \"push\": {\"type\": \"boolean\"}\n",
    "                        },\n",
    "                        \"required\": [\"sla_hours\", \"volume\", \"push\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        self.tool_used = False  # ðŸ”’ critical flag\n",
    "\n",
    "    # --------------------\n",
    "    # Tool implementation\n",
    "    # --------------------\n",
    "    def choose_transport(self, sla_hours: int, volume: int, push: bool) -> dict:\n",
    "        if push and sla_hours >= 2 and volume >= 1000:\n",
    "            return {\n",
    "                \"decision\": \"Kafka\",\n",
    "                \"reason\": \"High volume, async SLA, push-based architecture\"\n",
    "            }\n",
    "        return {\n",
    "            \"decision\": \"API\",\n",
    "            \"reason\": \"Lower volume or synchronous requirement\"\n",
    "        }\n",
    "\n",
    "    def _execute_tool(self, name: str, args: dict) -> dict:\n",
    "        if name == \"choose_transport\":\n",
    "            return self.choose_transport(**args)\n",
    "        raise ValueError(f\"Unknown tool: {name}\")\n",
    "\n",
    "    # --------------------\n",
    "    # Agent loop\n",
    "    # --------------------\n",
    "    def run(self, user_input: str, max_steps: int = 5) -> str:\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # run agent on-loop to assist a conversation\n",
    "        for step in range(max_steps):\n",
    "            # use llm-client's chat completion service and models to generate a response\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=self.messages,\n",
    "                # allowing tool usage at max once - this will ensure agent\n",
    "                # doesn't try infinite tool calls\n",
    "                tools=self.tools if not self.tool_used else None,\n",
    "                tool_choice=\"auto\" if not self.tool_used else \"none\",\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            message = response.choices[0].message\n",
    "\n",
    "            # ðŸ”§ Tool call (only once)\n",
    "            if message.tool_calls and not self.tool_used:\n",
    "                self.tool_used = True\n",
    "\n",
    "                tool_call = message.tool_calls[0]\n",
    "                tool_name = tool_call.function.name\n",
    "                tool_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "                print(f\"\\nðŸ”§ Tool called: {tool_name}({tool_args})\")\n",
    "\n",
    "                result = self._execute_tool(tool_name, tool_args)\n",
    "\n",
    "                self.messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"content\": json.dumps(result)\n",
    "                })\n",
    "\n",
    "            # âœ… Final answer\n",
    "            else:\n",
    "                self.messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": message.content\n",
    "                })\n",
    "                return message.content\n",
    "\n",
    "        raise RuntimeError(\"Agent did not completed within max steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1fa44c2-f397-4db5-a408-f77a0b3197c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Tool called: choose_transport({'push': True, 'sla_hours': 4, 'volume': 1000})\n",
      "\n",
      "âœ… FINAL OUTPUT:\n",
      " FINAL ANSWER: Based on the provided information, the recommended system design is Kafka. This is because Kafka is well-suited for handling high volumes of data, has an asynchronous SLA, and is designed for push-based architectures, making it a good fit for the described use case with thousands of comparisons per run and a downstream SLA of 4 hours.\n"
     ]
    }
   ],
   "source": [
    "agent = ToolAgent()\n",
    "\n",
    "output = agent.run(\n",
    "    \"Push-based system, thousands of comparisons per run, \"\n",
    "    \"downstream SLA is 4 hours\"\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… FINAL OUTPUT:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ad9dac-82c9-4ba0-a23e-ba10182c0de2",
   "metadata": {},
   "source": [
    "# $ Using Abstract Class to call multiple tools cleanly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dbeb1f-8f65-4186-9c73-8b27d552ba2e",
   "metadata": {},
   "source": [
    "## 1. Create a base class to define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4eae84d5-e49c-49ea-bda9-1c820b41a802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class BaseTool(ABC):\n",
    "    name: str\n",
    "    description: str\n",
    "\n",
    "    @abstractmethod\n",
    "    def schema(self) -> dict:\n",
    "        \"\"\"Return OpenAI/Groq-compatible tool schema\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def run(self, **kwargs) -> dict:\n",
    "        \"\"\"Execute tool\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b4b068-4032-49d8-a5f2-fabff828eee9",
   "metadata": {},
   "source": [
    "## 2. Create each tool using abstract class\n",
    "\n",
    "* Each tool should inherit and override base class functions\n",
    "* This abstract class ensures that all the functional tools are written with uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "320fcd7f-d634-4dcc-a404-522dde39def3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ChooseTransportTool(BaseTool):\n",
    "    name = \"choose_transport\"\n",
    "    description = \"Decide Kafka vs API based on system constraints\"\n",
    "\n",
    "    def schema(self) -> dict:\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": self.name,\n",
    "                \"description\": self.description,\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"sla_hours\": {\"type\": \"integer\"},\n",
    "                        \"volume\": {\"type\": \"integer\"},\n",
    "                        \"push\": {\"type\": \"boolean\"}\n",
    "                    },\n",
    "                    \"required\": [\"sla_hours\", \"volume\", \"push\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def run(self, sla_hours: int, volume: int, push: bool) -> dict:\n",
    "        if push and sla_hours >= 2 and volume >= 1000:\n",
    "            return {\"transport\": \"Kafka\"}\n",
    "        return {\"transport\": \"API\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106a4a2-cd26-4225-b070-248139bc90f1",
   "metadata": {},
   "source": [
    "## 3. Create a tool registry to call all the tools\n",
    "\n",
    "* Clean separation\n",
    "* Single source of truth\n",
    "* Easy extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9fe61e55-7439-45ad-8f21-32955f6226b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToolRegistry:\n",
    "    def __init__(self, tools: list[BaseTool]):\n",
    "        self._tools = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def schemas(self) -> list[dict]:\n",
    "        \"\"\"Expose all tool schemas to LLM\"\"\"\n",
    "        return [tool.schema() for tool in self._tools.values()]\n",
    "\n",
    "    def execute(self, name: str, args: dict) -> dict:\n",
    "        if name not in self._tools:\n",
    "            raise ValueError(f\"Tool '{name}' not registered\")\n",
    "        return self._tools[name].run(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4f6f7-1060-42f6-b334-f46566c3ee19",
   "metadata": {},
   "source": [
    "## 4. Agent Using tool registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c2f6375-77a6-4cde-bb7d-24250659b76a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToolAgent:\n",
    "    def __init__(self, tool_registry, client, model):\n",
    "        self.tool_registry = tool_registry\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.tool_used = False\n",
    "\n",
    "        self.messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an expert agent. \"\n",
    "                    \"Call at most one tool, then conclude.\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def run(self, user_input: str):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            tools=self.tool_registry.schemas(),\n",
    "            tool_choice=\"auto\",\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        msg = response.choices[0].message\n",
    "\n",
    "        if msg.tool_calls:\n",
    "            call = msg.tool_calls[0]\n",
    "            result = self.tool_registry.execute(\n",
    "                call.function.name,\n",
    "                json.loads(call.function.arguments)\n",
    "            )\n",
    "\n",
    "            self.messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": call.id,\n",
    "                \"content\": json.dumps(result)\n",
    "            })\n",
    "\n",
    "            final = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=self.messages,\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            return final.choices[0].message.content\n",
    "\n",
    "        return msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0911b591-e9b6-4923-af5d-588764b2a6a0",
   "metadata": {},
   "source": [
    "## 5. Run the agentic-frameowrk with multiple tools and memory\n",
    "\n",
    "Why this design scales\n",
    "\n",
    "* One abstract contract\n",
    "* Self-contained tools\n",
    "* Zero agent coupling\n",
    "* Easy testing (tool.run() standalone)\n",
    "* Auto-discoverable schemas\n",
    "* Production-ready\n",
    "\n",
    "> NOTE: This is exactly how internal agent platforms are structured (minus internal infra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea141aa2-fb2c-401f-b714-9459bfb7ffcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided, it seems like you're describing a system that requires a high-volume data processing solution with a Service Level Agreement (SLA) of 4 hours. Given the characteristics of a push-based system and the need to handle high volumes of data within a specific time frame, utilizing Apache Kafka as the transport mechanism is a suitable choice.\n",
      "\n",
      "Kafka is designed for high-throughput and provides low-latency, fault-tolerant, and scalable data processing. It's particularly well-suited for real-time data processing and can handle large volumes of data, making it an appropriate tool for systems that require efficient data handling and processing within a specified time window, such as a 4-hour SLA.\n",
      "\n",
      "Therefore, the conclusion is that using Kafka as the transport mechanism can help meet the requirements of a push-based system with high volume and a 4-hour SLA by ensuring efficient, scalable, and reliable data processing.\n",
      "For a payment gateway system with a push-based architecture, high volume, and a Service Level Agreement (SLA) of 4 hours, here are some details:\n",
      "\n",
      "1. **Architecture**: The system will utilize a Kafka cluster as the message broker to handle high-volume transactions. Kafka's distributed architecture and fault-tolerant design make it an ideal choice for handling large amounts of data.\n",
      "2. **Data Processing**: The system will use a combination of Kafka Streams and Apache Spark for real-time data processing. Kafka Streams will handle the initial processing of transactions, while Apache Spark will be used for more complex data processing and analytics.\n",
      "3. **Storage**: The system will use a distributed database such as Apache Cassandra or MongoDB to store transaction data. These databases are designed to handle high volumes of data and provide low-latency access.\n",
      "4. **Security**: The system will implement robust security measures, including encryption, authentication, and access control. This will ensure that sensitive payment information is protected and secure.\n",
      "\n",
      "As for the news on PayPal stock price, as of February 1, 2026, the stock price of PayPal Holdings, Inc. (PYPL) is $123.45 USD. Please note that this information is subject to change and may not reflect the current market price.\n",
      "\n",
      "Here's a sample news article:\n",
      "\n",
      "\"PayPal Stock Price Surges as Company Reports Strong Q4 Earnings\n",
      "February 1, 2026 - PayPal Holdings, Inc. (PYPL) reported strong fourth-quarter earnings, beating analyst expectations. The company's revenue grew by 20% year-over-year, driven by increased adoption of its payment services. As a result, the stock price of PayPal surged by 5% to $123.45 USD, outperforming the broader market.\"\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "from tools import *\n",
    "\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "# create a list of tools that you want our agent to use\n",
    "tools = [\n",
    "    ChooseTransportTool(),\n",
    "    EstimateCostTool(),\n",
    "    EstimateLatencyTool(),\n",
    "    FetchNewsTool(),\n",
    "    PlotTopicFrequencyTool(),\n",
    "    AnalyzeSentimentTool(),\n",
    "    # Add more tools here\n",
    "]\n",
    "\n",
    "# register all the tools to ToolRegistry\n",
    "registry = ToolRegistry(tools)\n",
    "\n",
    "# call the tool integrated agent for response generation task\n",
    "agent = ToolAgent(\n",
    "    tool_registry=registry,\n",
    "    client=client,\n",
    "    model=\"llama-3.3-70b-versatile\"\n",
    ")\n",
    "\n",
    "print(agent.run(\"Push-based system, high volume, SLA 4 hours\"))\n",
    "print(agent.run(\"Give details for a payment gateway system and give me news on Paypal stock price with date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5ff6568-70fe-4589-8960-c7fb004c5854",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an expert agent. Call at most one tool, then conclude.'},\n",
       " {'role': 'user', 'content': 'Push-based system, high volume, SLA 4 hours'},\n",
       " {'role': 'tool',\n",
       "  'tool_call_id': '40cgne20r',\n",
       "  'content': '{\"transport\": \"Kafka\"}'},\n",
       " {'role': 'user',\n",
       "  'content': 'Give details for a payment gateway system and give me news on Paypal stock price with date'},\n",
       " {'role': 'tool',\n",
       "  'tool_call_id': '4psve70qk',\n",
       "  'content': '{\"transport\": \"Kafka\"}'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed894781-7397-4176-a669-7ae265b8fc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
